{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image is too large\n",
    "\n",
    "[Resize All Images in a Linux Folder](https://linuxhint.com/resize-all-images-folder-linux/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir -p \"../images\"\n",
    "\n",
    "for img in *.jpg \n",
    "do\n",
    "    convert -resize 13.333% \"$img\" \"../images/$img\"\n",
    "done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this can Ignore Aspect Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir -p \"../images\"\n",
    "\n",
    "for img in *.jpg \n",
    "do\n",
    "    convert -resize 640x384\\! \"$img\" \"../images/$img\"\n",
    "done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfrecord create\n",
    "this dataset is different from COCO, so we need other method to create tfrecord.  \n",
    "To follow the same workflow, make dataset to COCO format seem to be a good method. And make it able to use by other framework.    \n",
    "I find this tutorial [Convert any dataset to COCO object detection format with SAHI](https://medium.com/codable/convert-any-dataset-to-coco-object-detection-format-with-sahi-95349e1fe2b7).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python yolo2coco.py --root_dir \"../obj/\" --random_split --split_ratio \"0.8:0.1:0.1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then as usual but small modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_DATA_DIR='../obj/images'\n",
    "TRAIN_ANNOTATION_FILE_DIR='../obj/annotations/train.json'\n",
    "OUTPUT_TFRECORD_TRAIN='../cabbage_coco_tfrecords/train'\n",
    "\n",
    "# Need to provide\n",
    "  # 1. image_dir: where images are present\n",
    "  # 2. object_annotations_file: where annotations are listed in json format\n",
    "  # 3. output_file_prefix: where to write output convered TFRecords files\n",
    "python -m official.vision.data.create_coco_tf_record --logtostderr \\\n",
    "  --image_dir=${TRAIN_DATA_DIR} \\\n",
    "  --object_annotations_file=${TRAIN_ANNOTATION_FILE_DIR} \\\n",
    "  --output_file_prefix=$OUTPUT_TFRECORD_TRAIN \\\n",
    "  --num_shards=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "VALID_DATA_DIR='../obj/images'\n",
    "VALID_ANNOTATION_FILE_DIR='../obj/annotations/val.json'\n",
    "OUTPUT_TFRECORD_VALID='../cabbage_coco_tfrecords/valid'\n",
    "\n",
    "python -m official.vision.data.create_coco_tf_record --logtostderr \\\n",
    "  --image_dir=$VALID_DATA_DIR \\\n",
    "  --object_annotations_file=$VALID_ANNOTATION_FILE_DIR \\\n",
    "  --output_file_prefix=$OUTPUT_TFRECORD_VALID \\\n",
    "  --num_shards=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "TEST_DATA_DIR='../obj/images'\n",
    "TEST_ANNOTATION_FILE_DIR='../obj/annotations/test.json'\n",
    "OUTPUT_TFRECORD_TEST='../cabbage_coco_tfrecords/test'\n",
    "\n",
    "python -m official.vision.data.create_coco_tf_record --logtostderr \\\n",
    "  --image_dir=$TEST_DATA_DIR \\\n",
    "  --object_annotations_file=$TEST_ANNOTATION_FILE_DIR \\\n",
    "  --output_file_prefix=$OUTPUT_TFRECORD_TEST \\\n",
    "  --num_shards=1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pprint\n",
    "import tempfile\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from six import BytesIO\n",
    "from IPython import display\n",
    "from urllib.request import urlopen\n",
    "\n",
    "tf.print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbit\n",
    "import tensorflow_models as tfm\n",
    "\n",
    "from official.core import exp_factory\n",
    "from official.core import config_definitions as cfg\n",
    "from official.vision.serving import export_saved_model_lib # only served lib !\n",
    "from official.vision.ops.preprocess_ops import normalize_image\n",
    "from official.vision.ops.preprocess_ops import resize_and_crop_image\n",
    "from official.vision.utils.object_detection import visualization_utils\n",
    "from official.vision.dataloaders.tf_example_decoder import TfExampleDecoder\n",
    "\n",
    "from official.vision.ops.nms import sorted_non_max_suppression_padded\n",
    "# import official.projects.centernet.common.registry_imports\n",
    "from official.projects.centernet.configs.centernet import centernet_hourglass_coco\n",
    "\n",
    "import official.projects.yolo.common.registry_imports\n",
    "\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4) # Set Pretty Print Indentation\n",
    "print(tf.__version__) # Check the version of tensorflow used\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Vision modules: \", dir(tfm.vision))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Retinanet Resnet FPN COCO model for custom dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if your image is not square e.g. 512x512, 256x256 you need to set more training step.  \n",
    "for square image: 2000+ step  \n",
    "for rectangle image: 3000+ step\n",
    "\n",
    "for other model e.g. 'retinanet_spinenet_coco', 'retinanet_maxvit_coco', 'detr_coco', 'fasterrcnn_resnetfpn_coco', 'maskrcnn_resnetfpn_coco', 'pix2seq_r50_coco', 'centernet_hourglass_coco', 'deep_mask_head_rcnn_spinenet_coco', 'panoptic_deeplab_resnet_coco', 'panoptic_fpn_coco'  \n",
    "'coco_yolov7'\n",
    "\n",
    "plz refer [configs](https://github.com/tensorflow/models/tree/20cd431c7940489ad807706e046add604e43e75b/official/vision/configs) xxxnext_test.py or [repo:tensorflow/models register_config_factory](https://github.com/search?q=repo%3Atensorflow%2Fmodels%20register_config_factory&type=code) find regist name.\n",
    "\n",
    "\n",
    "'centernet_hourglass_coco' is at official/projects/centernet/configs/centernet.py.\n",
    "\n",
    "---\n",
    "\n",
    "Or only model name in `official/vision/serving/detection_test.py` have pretrained model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_input_path = 'bccd_coco_tfrecords/train-00000-of-00001.tfrecord'\n",
    "# valid_data_input_path = 'bccd_coco_tfrecords/valid-00000-of-00001.tfrecord'\n",
    "# test_data_input_path = 'bccd_coco_tfrecords/test-00000-of-00001.tfrecord'\n",
    "# model_dir = 'trained_model/'\n",
    "# export_dir ='exported_model/'\n",
    "\n",
    "# model_name='retinanet_spinenet_coco'\n",
    "#model_name='maskrcnn_resnetfpn_coco' # no mask data\n",
    "model_name='fasterrcnn_resnetfpn_coco'\n",
    "\n",
    "\n",
    "train_data_input_path = '../cabbage_coco_tfrecords/train-00000-of-00001.tfrecord'\n",
    "valid_data_input_path = '../cabbage_coco_tfrecords/valid-00000-of-00001.tfrecord'\n",
    "test_data_input_path = '../cabbage_coco_tfrecords/test-00000-of-00001.tfrecord'\n",
    "\n",
    "model_dir = '../cbg_{}_trained_model/'.format(model_name) #this path need to modify for each model #https://github.com/tensorflow/models/issues/9703\n",
    "export_dir ='../cbg_{}_exported_model/'.format(model_name)\n",
    "\n",
    "\n",
    "\n",
    "exp_config = exp_factory.get_exp_config(model_name)\n",
    "# exp_config=centernet_hourglass_coco()\n",
    "\n",
    "batch_size = 2 #8\n",
    "num_classes = 1 #3\n",
    "\n",
    "# HEIGHT, WIDTH = 256, 256\n",
    "WIDTH, HEIGHT=640, 384 #288,384\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE = [HEIGHT, WIDTH, 3]\n",
    "\n",
    "# Backbone config.\n",
    "exp_config.task.freeze_backbone = False\n",
    "exp_config.task.annotation_file = ''\n",
    "\n",
    "# Model config.\n",
    "exp_config.task.model.input_size = IMG_SIZE\n",
    "exp_config.task.model.num_classes = num_classes + 1\n",
    "#this is for retinanet, maskrcnn have no this setting.\n",
    "# exp_config.task.model.detection_generator.tflite_post_processing.max_classes_per_detection = exp_config.task.model.num_classes\n",
    "\n",
    "# Training data config.\n",
    "exp_config.task.train_data.input_path = train_data_input_path\n",
    "exp_config.task.train_data.dtype = 'float32'\n",
    "exp_config.task.train_data.global_batch_size = batch_size\n",
    "exp_config.task.train_data.parser.aug_rand_hflip=True\n",
    "exp_config.task.train_data.parser.aug_rand_vflip=True\n",
    "exp_config.task.train_data.parser.aug_scale_max = 1.2\n",
    "exp_config.task.train_data.parser.aug_scale_min = 0.8\n",
    "\n",
    "# Validation data config.\n",
    "exp_config.task.validation_data.input_path = valid_data_input_path\n",
    "exp_config.task.validation_data.dtype = 'float32'\n",
    "exp_config.task.validation_data.global_batch_size = batch_size\n",
    "\n",
    "#--------------------------------------------\n",
    "logical_device_names = [logical_device.name for logical_device in tf.config.list_logical_devices()]\n",
    "\n",
    "if 'GPU' in ''.join(logical_device_names):\n",
    "  print('This may be broken in Colab.')\n",
    "  device = 'GPU'\n",
    "elif 'TPU' in ''.join(logical_device_names):\n",
    "  print('This may be broken in Colab.')\n",
    "  device = 'TPU'\n",
    "else:\n",
    "  print('Running on CPU is slow, so only train for a few steps.')\n",
    "  device = 'CPU'\n",
    "\n",
    "\n",
    "train_steps = 5000\n",
    "exp_config.trainer.steps_per_loop = 100 # steps_per_loop = num_of_training_examples // train_batch_size\n",
    "\n",
    "exp_config.trainer.summary_interval = 100\n",
    "exp_config.trainer.checkpoint_interval = 100\n",
    "exp_config.trainer.validation_interval = 100\n",
    "exp_config.trainer.validation_steps =  100 # validation_steps = num_of_validation_examples // eval_batch_size\n",
    "exp_config.trainer.train_steps = train_steps\n",
    "exp_config.trainer.optimizer_config.warmup.linear.warmup_steps = 100\n",
    "exp_config.trainer.optimizer_config.learning_rate.type = 'cosine'\n",
    "exp_config.trainer.optimizer_config.learning_rate.cosine.decay_steps = train_steps\n",
    "exp_config.trainer.optimizer_config.learning_rate.cosine.initial_learning_rate = 0.1\n",
    "exp_config.trainer.optimizer_config.warmup.linear.warmup_learning_rate = 0.05\n",
    "\n",
    "#print rst\n",
    "pp.pprint(exp_config.as_dict())\n",
    "# display.Javascript('google.colab.output.setIframeHeight(\"500px\");')\n",
    "\n",
    "# distribution strategy\n",
    "if exp_config.runtime.mixed_precision_dtype == tf.float16:\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "if 'GPU' in ''.join(logical_device_names):\n",
    "  distribution_strategy = tf.distribute.MirroredStrategy()\n",
    "elif 'TPU' in ''.join(logical_device_names):\n",
    "  tf.tpu.experimental.initialize_tpu_system()\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='/device:TPU_SYSTEM:0')\n",
    "  distribution_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "  print('Warning: this will be really slow.')\n",
    "  distribution_strategy = tf.distribute.OneDeviceStrategy(logical_device_names[0])\n",
    "\n",
    "print('Done')\n",
    "\n",
    "# Create the Task object \n",
    "with distribution_strategy.scope():\n",
    "  task = tfm.core.task_factory.get_task(exp_config.task, logging_dir=model_dir)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a batch of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in task.build_inputs(exp_config.task.train_data).take(1):\n",
    "  print()\n",
    "  print(f'images.shape: {str(images.shape):16}  images.dtype: {images.dtype!r}')\n",
    "  print(f'labels.keys: {labels.keys()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_index={\n",
    "    # 1: {\n",
    "    #     'id': 1,\n",
    "    #     'name': 'Platelets'\n",
    "    #    },\n",
    "    # 2: {\n",
    "    #     'id': 2,\n",
    "    #     'name': 'RBC'\n",
    "    #    },\n",
    "    # 3: {\n",
    "    #     'id': 3,\n",
    "    #     'name': 'WBC'\n",
    "    #    }\n",
    "# }\n",
    "category_index={\n",
    "  1:{\n",
    "    'id':1,\n",
    "    'name':'Cabbage'\n",
    "  }\n",
    "}\n",
    "tf_ex_decoder = TfExampleDecoder()\n",
    "\n",
    "\n",
    "def show_batch(raw_records, num_of_examples):\n",
    "  plt.figure(figsize=(20, 20))\n",
    "  use_normalized_coordinates=True\n",
    "  min_score_thresh = 0.30\n",
    "  for i, serialized_example in enumerate(raw_records):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    decoded_tensors = tf_ex_decoder.decode(serialized_example)\n",
    "    image = decoded_tensors['image'].numpy().astype('uint8')\n",
    "    scores = np.ones(shape=(len(decoded_tensors['groundtruth_boxes'])))\n",
    "    visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image,\n",
    "        decoded_tensors['groundtruth_boxes'].numpy(),\n",
    "        decoded_tensors['groundtruth_classes'].numpy().astype('int'),\n",
    "        scores,\n",
    "        category_index=category_index,\n",
    "        use_normalized_coordinates=use_normalized_coordinates,\n",
    "        max_boxes_to_draw=200,\n",
    "        min_score_thresh=min_score_thresh,\n",
    "        agnostic_mode=False,\n",
    "        instance_masks=None,\n",
    "        line_thickness=1)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Image-{i+1}')\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 20\n",
    "num_of_examples = 3\n",
    "\n",
    "raw_records = tf.data.TFRecordDataset(\n",
    "    exp_config.task.train_data.input_path).shuffle(\n",
    "        buffer_size=buffer_size).take(num_of_examples)\n",
    "show_batch(raw_records, num_of_examples)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, eval_logs = tfm.core.train_lib.run_experiment(\n",
    "    distribution_strategy=distribution_strategy,\n",
    "    task=task,\n",
    "    mode='train_and_eval',\n",
    "    params=exp_config,\n",
    "    model_dir=model_dir,\n",
    "    run_post_eval=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# %tensorboard --logdir '../cbg_hourglass_trained_model/' --port 6009\n",
    "%tensorboard --logdir $model_dir --port 6013\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_saved_model_lib.export_inference_graph(\n",
    "    input_type='image_tensor',\n",
    "    batch_size=1,\n",
    "    input_image_size=[HEIGHT, WIDTH],\n",
    "    params=exp_config,\n",
    "    checkpoint_path=tf.train.latest_checkpoint(model_dir),\n",
    "    export_dir=export_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "  \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "  Puts image into numpy array to feed into tensorflow graph.\n",
    "  Note that by convention we put it into a numpy array with shape\n",
    "  (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "  Args:\n",
    "    path: the file path to the image\n",
    "\n",
    "  Returns:\n",
    "    uint8 numpy array with shape (img_height, img_width, 3)\n",
    "  \"\"\"\n",
    "  image = None\n",
    "  if(path.startswith('http')):\n",
    "    response = urlopen(path)\n",
    "    image_data = response.read()\n",
    "    image_data = BytesIO(image_data)\n",
    "    image = Image.open(image_data)\n",
    "  else:\n",
    "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "def build_inputs_for_object_detection(image, input_image_size):\n",
    "  \"\"\"Builds Object Detection model inputs for serving.\"\"\"\n",
    "  image, _ = resize_and_crop_image(\n",
    "      image,\n",
    "      input_image_size,\n",
    "      padded_size=input_image_size,\n",
    "      aug_scale_min=1.0,\n",
    "      aug_scale_max=1.0)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_examples = 3\n",
    "\n",
    "test_ds = tf.data.TFRecordDataset(\n",
    "    test_data_input_path).take(\n",
    "        num_of_examples)\n",
    "show_batch(test_ds, num_of_examples)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "imported = tf.saved_model.load(export_dir)\n",
    "model_fn = imported.signatures['serving_default']\n",
    "\n",
    "input_image_size = (HEIGHT, WIDTH)\n",
    "plt.figure(figsize=(20, 20))\n",
    "min_score_thresh = 0.40 # Change minimum score for threshold to see all bounding boxes confidences.\n",
    "\n",
    "for i, serialized_example in enumerate(test_ds):\n",
    "  plt.subplot(1, 3, i+1)\n",
    "  decoded_tensors = tf_ex_decoder.decode(serialized_example)\n",
    "  image = build_inputs_for_object_detection(decoded_tensors['image'], input_image_size)\n",
    "  image = tf.expand_dims(image, axis=0)\n",
    "  image = tf.cast(image, dtype = tf.uint8)\n",
    "  image_np = image[0].numpy()\n",
    "  \n",
    "  result = model_fn(image) #evaluate\n",
    "\n",
    "  # nms_scores,nms_proposals=  sorted_non_max_suppression_padded(\n",
    "  #   scores=result['detection_scores'], \n",
    "  #   boxes=result['detection_boxes'],\n",
    "  #   max_output_size=100,\n",
    "  #   iou_threshold=0.1 #cabbage impossible touch\n",
    "  #   )\n",
    "  #'boxes', 'classes', 'confidence'\n",
    "  selected_indices=tf.image.non_max_suppression(\n",
    "    boxes=result['detection_boxes'][0],\n",
    "    scores=result['detection_scores'][0], \n",
    "    max_output_size=100,\n",
    "    iou_threshold=0.01,\n",
    "    score_threshold=min_score_thresh, #float('-inf'),\n",
    "    name=None\n",
    "  )\n",
    "  nms_proposals = tf.gather(result['detection_boxes'][0], selected_indices)\n",
    "  nms_scores=tf.gather(result['detection_scores'][0], selected_indices)\n",
    "  nms_classes=tf.gather(result['detection_classes'][0], selected_indices)\n",
    "\n",
    "\n",
    "  visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      nms_proposals.numpy(),\n",
    "      # result['detection_boxes'][0].numpy(),\n",
    "      # selected_boxes\n",
    "\n",
    "      # result['detection_classes'][0].numpy().astype(int),\n",
    "      nms_classes.numpy().astype(int),\n",
    "\n",
    "      # result['detection_scores'][0].numpy(),\n",
    "      nms_scores.numpy(),\n",
    "\n",
    "      category_index=category_index,\n",
    "      use_normalized_coordinates=False,\n",
    "      max_boxes_to_draw=100,\n",
    "      min_score_thresh=min_score_thresh,\n",
    "      agnostic_mode=False,\n",
    "      instance_masks=None,\n",
    "      line_thickness=2)\n",
    "  plt.imshow(image_np)\n",
    "  plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nms_scores\n",
    "# nms_proposals\n",
    "# result['detection_scores'][0]\n",
    "# result.keys()\n",
    "# # result['boxes']\n",
    "# result['confidence']+0.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
